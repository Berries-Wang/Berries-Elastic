# 索引分片过多，导致Search Rate过高
> 先阅读一下: [999.Lessons/000.Lucene&ElasticSearch架构.md](../../../999.Lessons/000.Lucene&ElasticSearch架构.md)

### 背景
```txt
    a. 主分片数量: 128 
    b. 副本分片数量: 1 
    c. 文档总数: 27亿+
    d. 根据用户标识分片(分片id = open_id-A存在? open_id-A : open_id-B)
    e. 存储的数据： 订单

  用户在不同的app_key下有不同的open_id

  查询场景1: 根据卖家ID分页查询订单数据
  查询场景2: 根据卖家ID和买家open_id-B查询买家在卖家下面的订单数据
```

### 分析 & 处理方案
```txt
## 分析
根据ES查询原理可知(并发查询所有的segement)
对于查询场景1 场景2 ， 都需要查询所有的segement ,这就导致原本查询的QPS为100，在ES内部被放大100倍+,
所以造成了ES系统负载过高，持续告警

## 处理方案
### 对于查询场景1
  在请求入参中，指定shard_id , 循环查询所有的shard
    SearchRequest searchRequest = new SearchRequest(__indice_name__);
    // 指定shard_id
    searchRequest.preference("_shards:" + request.getShardId());

### 对于查询场景2
  在查询时，通过open_id-B 转换得到 open_id-A , 再指定路由规则通过open_id-A查询订单列表
   searchRequest.routing(${open_id-A}, ${open_id-B});

怎么查看索引在ES内部的查询速率:
  Kibana -> Stack Monitoring -> Elasticsearch Overview -> Indices  Search Rate就是查询速率
```

### 优化效果
```txt
原(<=200QPS) --优化后--> 2500+QPS(ES未报负载过高,未继续压测) 
   性能大约提升了10倍+
```